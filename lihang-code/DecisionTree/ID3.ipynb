{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter\n",
    "import math\n",
    "from math import log\n",
    "\n",
    "import sys\n",
    "import pprint\n",
    "\n",
    "\n",
    "# table 5.1 from book\n",
    "def create_data():\n",
    "    datasets = [['teen', 'no', 'no', 'intermediate', 'no'],\n",
    "               ['teen', 'no', 'no', 'good', 'no'],\n",
    "               ['teen', 'yes', 'no', 'good', 'yes'],\n",
    "               ['teen', 'yes', 'yes', 'intermediate', 'yes'],\n",
    "               ['teen', 'no', 'no', 'intermediate', 'no'],\n",
    "               ['middle-age', 'no', 'no', 'intermediate', 'no'],\n",
    "               ['middle-age', 'no', 'no', 'good', 'no'],\n",
    "               ['middle-age', 'yes', 'yes', 'good', 'yes'],\n",
    "               ['middle-age', 'no', 'yes', 'very-good', 'yes'],\n",
    "               ['middle-age', 'no', 'yes', 'very-good', 'yes'],\n",
    "               ['old', 'no', 'yes', 'very-good', 'yes'],\n",
    "               ['old', 'no', 'yes', 'good', 'yes'],\n",
    "               ['old', 'yes', 'no', 'good', 'yes'],\n",
    "               ['old', 'yes', 'no', 'very-good', 'yes'],\n",
    "               ['old', 'no', 'no', 'intermediate', 'no'],\n",
    "               ['teen', 'no', 'no', 'intermediate', 'yes']\n",
    "               ]\n",
    "    labels = ['age', 'have job', 'own house', 'credit situation', 'type']\n",
    "    \n",
    "    return datasets, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_node(node, depth=0):  \n",
    "    if node.splitting_feature_id is None:\n",
    "        print(depth, (node.splitting_feature_id, node.splitting_feature_value, \n",
    "                      node.class_label, len(node.data)))\n",
    "    else:\n",
    "        print(depth, (node.splitting_feature_id, node.splitting_feature_value))\n",
    "        for c in node.child:\n",
    "            print_node(c, depth+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, splitting_feature_id=None, class_label=None, data=None, \n",
    "                 splitting_feature_value=None):\n",
    "        self.splitting_feature_id = splitting_feature_id     # the splitting point\n",
    "        self.splitting_feature_value = splitting_feature_value   # feature value\n",
    "        self.class_label = class_label               # class label, only leaf own \n",
    "        self.data = data                             # labels of samples, only leaf own\n",
    "        self.child = []                              # child node\n",
    "    \n",
    "    \n",
    "    def add_node(self, node):\n",
    "        self.child.append(node)\n",
    "        \n",
    "    def predict(self, test_features):\n",
    "        if self.class_label is not None:\n",
    "            return self.class_label\n",
    "        for c in self.child:\n",
    "            if c.splitting_feature_value == test_features[self.splitting_feature_id]:\n",
    "                return c.predict(test_features)\n",
    "\n",
    "class DTree:\n",
    "    def __init__(self, epsilon=0, alpha=0):\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.tree = Node()\n",
    "        \n",
    "    def cal_entropy(self, datasets):\n",
    "        n = len(datasets)\n",
    "        label_count = {}\n",
    "        # get distribution(Pi)\n",
    "        for i in range(n):\n",
    "            label = datasets[i][-1]\n",
    "            if label not in label_count:\n",
    "                label_count[label] = 0\n",
    "            label_count[label] += 1\n",
    "        # print(label_count) {'no': 6, 'yes': 9}\n",
    "        empirical_entropy = -sum([(p/n) * log(p/n, 2) for p in label_count.values()])\n",
    "\n",
    "        return empirical_entropy\n",
    "\n",
    "    # empirical conditional entropy\n",
    "    def cal_conditional_entropy(self, datasets, axis=0):\n",
    "        n = len(datasets)\n",
    "        feature_sets = {}\n",
    "        for i in range(n):\n",
    "            feature = datasets[i][axis]\n",
    "            if feature not in feature_sets:\n",
    "                feature_sets[feature] = []\n",
    "            feature_sets[feature].append(datasets[i])\n",
    "\n",
    "        empirical_conditional_entropy = sum([(len(p)/n) * self.cal_entropy(p)\n",
    "                                            for p in feature_sets.values()])\n",
    "\n",
    "        return empirical_conditional_entropy\n",
    "\n",
    "    # information gain\n",
    "    def info_gain(self, entropy, con_entropy):\n",
    "        return entropy - con_entropy\n",
    "\n",
    "    def get_info_gain(self, datasets):\n",
    "        feature_count = len(datasets[0]) - 1\n",
    "        empirical_entropy = self.cal_entropy(datasets)\n",
    "        best_feature = []\n",
    "\n",
    "        for c in range(feature_count):\n",
    "            c_info_gain = self.info_gain(empirical_entropy, \n",
    "                                         self.cal_conditional_entropy(datasets, axis=c))\n",
    "            best_feature.append((c, c_info_gain))\n",
    "\n",
    "        best = max(best_feature, key=lambda x : x[-1])\n",
    "        # best : ((feature_id, feature_info_gain))\n",
    "        return best\n",
    "    \n",
    "    def train(self, train_data, node):\n",
    "        '''\n",
    "        Input : Dataset(DataFrame), Feature_set A, threshold epsilon\n",
    "        Output : T(Decision Tree)\n",
    "        '''\n",
    "        _ = train_data.iloc[:, :-1]\n",
    "        y_train = train_data.iloc[:, -1]\n",
    "        features = train_data.columns[:-1]\n",
    "        \n",
    "        # 1. if all the data in D belong to the same class C, \n",
    "        #    set T as single node and use C as the label, return T\n",
    "        if len(y_train.value_counts()) == 1:\n",
    "            node.class_label = y_train.iloc[0]\n",
    "            node.data = y_train\n",
    "            return \n",
    "        \n",
    "        # 2. if feature A is empty, set T as single node and use the most C as the label, \n",
    "        #    return T\n",
    "        if len(features) == 0:\n",
    "            node.class_label = y_train.value_counts().sort_values(ascending=False).index[0]\n",
    "            node.data = y_train\n",
    "            return \n",
    "        \n",
    "        # 3. calculate the largest inforamtion gain, use Ag to representative the best feature\n",
    "        max_feature_id, max_info_gain = self.get_info_gain(np.array(train_data))\n",
    "        max_feature_name = features[max_feature_id]\n",
    "     \n",
    "        # 4. if the information gain is smaller than threshold, set T as single node,\n",
    "        #    and use the most C as the label, return T \n",
    "        if max_info_gain <= self.epsilon:\n",
    "            node.class_label = y_train.value_counts().sort_values(ascending=False).index[0]\n",
    "            node.data = y_train\n",
    "            return \n",
    "    \n",
    "\n",
    "        # 5. splitting D according to every possible values in the feature A\n",
    "        feature_list = train_data[max_feature_name].value_counts().index\n",
    "        for Di in feature_list:\n",
    "            node.splitting_feature_id = max_feature_id\n",
    "            child = Node(splitting_feature_value = Di)\n",
    "            node.add_node(child)\n",
    "            sub_train_data = pd.DataFrame([list(i) for i in train_data.values if i[max_feature_id] == Di],\n",
    "                                         columns = train_data.columns)\n",
    "            \n",
    "            # 6. create tree recursively\n",
    "            self.train(sub_train_data, child)\n",
    "    \n",
    "    def fit(self, train_data):\n",
    "        self.train(train_data, self.tree)\n",
    "        \n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        return self.tree.predict(x_test)\n",
    "    \n",
    "\n",
    "    def find_parent(self, node, error_min):\n",
    "        '''\n",
    "        leaf nodes : class_label -> not None\n",
    "                     data -> not None\n",
    "                     splitting_feature_id -> None\n",
    "                     splitting_feature_value -> None\n",
    "                     child -> None\n",
    "        ---------------------------------------------\n",
    "        others :     class_label -> None\n",
    "                     data -> None\n",
    "                     splitting_feature_id -> not None\n",
    "                     splitting_feature_value -> not None(except root)\n",
    "                     child -> not None\n",
    "        '''\n",
    "        if node.splitting_feature_id is not None:  # if not the leaf nodes\n",
    "            # collect class_labels from child nodes\n",
    "            class_label = [c.class_label for c in node.child]\n",
    "            \n",
    "            # if all the child nodes are leaf nodes\n",
    "            if None not in class_label:  \n",
    "                # collect data from child nodes\n",
    "                child_data = []\n",
    "                for c in node.child:\n",
    "                    for d in list(c.data):\n",
    "                        child_data.append(d)\n",
    "                child_counter = Counter(child_data)\n",
    "\n",
    "                # copy the old node\n",
    "                old_child = node.child\n",
    "                old_splitting_feature_id = node.splitting_feature_id\n",
    "                old_class_label = node.class_label\n",
    "                old_data = node.data\n",
    "                \n",
    "                # pruning\n",
    "                node.splitting_feature_id = None\n",
    "                node.class_label = child_counter.most_common(1)[0][0]\n",
    "                node.data = child_data\n",
    "                \n",
    "                error_after_pruning = self.c_error()\n",
    "                # if error_after_pruning <= error_min, it is worth to pruning\n",
    "                if error_after_pruning <= error_min:\n",
    "                    error_min = error_after_pruning\n",
    "                    return 1\n",
    "                # if not, recover the previous tree\n",
    "                else:\n",
    "                    node.child = old_child\n",
    "                    node.splitting_feature_id = old_splitting_feature_id\n",
    "                    node.class_label = old_class_label\n",
    "                    node.data = old_data\n",
    "                    \n",
    "            # if not all the child nodes are leaf nodes\n",
    "            else:\n",
    "                re = 0\n",
    "                i = 0\n",
    "                while i < len(node.child):\n",
    "                    # if the pruning action happend, \n",
    "                    # rescan the subtree since there are new leafs created\n",
    "                    if_re = self.find_parent(node.child[i], error_min)\n",
    "                    if if_re == 1:   \n",
    "                        re = 1\n",
    "                    elif if_re == 2:  \n",
    "                        i -= 1\n",
    "                    i += 1\n",
    "                if re:\n",
    "                    return 2\n",
    "\n",
    "        return 0\n",
    "                \n",
    "    \n",
    "    def find_leaf(self, node, leaf):   # find all leaf nodes\n",
    "        for t in node.child:\n",
    "            if t.class_label is not None:\n",
    "                leaf.append(t.data)\n",
    "            else:\n",
    "                for c in node.child:\n",
    "                    self.find_leaf(c, leaf)\n",
    "    \n",
    "    def c_error(self):   # calculate C_alpha_T for current subTree\n",
    "        leaf = []\n",
    "        self.find_leaf(self.tree, leaf)\n",
    "        leaf_num = [len(l) for l in leaf]\n",
    "        \n",
    "        # calculate empirical entropy for each leaf nodes\n",
    "        entropy = [self.cal_entropy(l) for l in leaf]\n",
    "        \n",
    "        # alpha * |T|\n",
    "        alpha_T = self.alpha * len(leaf_num)\n",
    "        \n",
    "        error = 0\n",
    "        C_alpha_T = 0 + alpha_T\n",
    "        \n",
    "        for Nt, Ht in zip(leaf_num, entropy):\n",
    "            C_T = Nt * Ht\n",
    "            error += C_T\n",
    "            \n",
    "        C_alpha_T += error   \n",
    "        \n",
    "        return C_alpha_T\n",
    "        \n",
    "    \n",
    "    def pruning(self, alpha=0):\n",
    "        if alpha:\n",
    "            self.alpha = alpha\n",
    "            \n",
    "        error_min = self.c_error()\n",
    "        \n",
    "        self.find_parent(self.tree, error_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (2, None)\n",
      "1 (1, 'no')\n",
      "2 (0, 'no')\n",
      "3 (3, 'teen')\n",
      "4 (None, 'intermediate', 'no', 3)\n",
      "4 (None, 'good', 'no', 1)\n",
      "3 (None, 'middle-age', 'no', 2)\n",
      "3 (None, 'old', 'no', 1)\n",
      "2 (None, 'yes', 'yes', 3)\n",
      "1 (None, 'yes', 'yes', 6)\n",
      "=============================\n",
      "0 (2, None)\n",
      "1 (1, 'no')\n",
      "2 (None, 'no', 'no', 7)\n",
      "2 (None, 'yes', 'yes', 3)\n",
      "1 (None, 'yes', 'yes', 6)\n"
     ]
    }
   ],
   "source": [
    "datasets, labels = create_data()\n",
    "data_df = pd.DataFrame(datasets, columns = labels)\n",
    "dt = DTree(epsilon=0)\n",
    "dt.fit(data_df)\n",
    "\n",
    "#result = dt.predict(['old', 'no', 'no', 'intermediate'])\n",
    "print_node(dt.tree)\n",
    "\n",
    "print('=============================')\n",
    "\n",
    "dt.pruning(alpha=0.5)\n",
    "print_node(dt.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lab\n",
    "a = 0\n",
    "if a:\n",
    "    print('here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
